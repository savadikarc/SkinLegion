{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_HEIGHT = 225\n",
    "IM_WIDTH = 300\n",
    "BATCH_SIZE = 16\n",
    "SEED = 473536\n",
    "akiec_location = '/home/ubuntu/Data/Skin/akiec_others/'\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "mean_im = Image.fromarray(np.load(akiec_location + 'mean_image_akiec_others.npz')['image']\n",
    "                          .astype(np.uint8)).resize((IM_WIDTH, IM_HEIGHT))\n",
    "\n",
    "mean_img = np.asarray(mean_im).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(im):\n",
    "    return (im - mean_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "img_gen = ImageDataGenerator(preprocessing_function=preprocess_image, rescale=1./255)\n",
    "\n",
    "train_gen = img_gen.flow_from_directory(akiec_location + 'train',\n",
    "                                       batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                       seed=SEED, target_size=(IM_HEIGHT, IM_WIDTH),\n",
    "                                        classes=['NON_AKIEC', 'AKIEC'],\n",
    "                                       class_mode='binary')\n",
    "\n",
    "val_gen = img_gen.flow_from_directory(akiec_location + 'val',\n",
    "                                       batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                       seed=SEED, target_size=(IM_HEIGHT, IM_WIDTH),\n",
    "                                      classes=['NON_AKIEC', 'AKIEC'],\n",
    "                                       class_mode='binary')\n",
    "\n",
    "test_gen = img_gen.flow_from_directory(akiec_location + 'test',\n",
    "                                       batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                       seed=SEED, target_size=(IM_HEIGHT, IM_WIDTH),\n",
    "                                       classes=['NON_AKIEC', 'AKIEC'],\n",
    "                                       class_mode='binary')\n",
    "\n",
    "y_train = train_gen.classes\n",
    "class_weight = compute_class_weight('balanced', np.unique(y_train), y_train.ravel())\n",
    "class_weights = {i: class_weight[i] for i in range(len(class_weight))}\n",
    "\n",
    "print('Class weights: ', class_weights)\n",
    "class_weights[1] += 2.\n",
    "print('Class weights: ', class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name = 'akiec_non_akiec'\n",
    "model_name = 'inceptionv3'\n",
    "num_hidden_units = 512\n",
    "\n",
    "num_hidden_units_1 = 512\n",
    "num_hidden_units_2 = 512\n",
    "\n",
    "filename = 'final' + data_name + '_' + model_name + '_' + str(num_hidden_units_1) + '_' + str(num_hidden_units_2) + '_' + '.h5'\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    return (true_positives / (predicted_positives + K.epsilon()))\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return (true_positives / (possible_positives + K.epsilon()))\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return ( 2*(precision * recall) / (precision + recall + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import (Dense, Activation, \n",
    "                          Dropout, BatchNormalization, \n",
    "                          GlobalAveragePooling2D)\n",
    "from keras import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "inc = InceptionV3(include_top=False, input_shape=(IM_HEIGHT, IM_WIDTH, 3), weights='imagenet')\n",
    "# x = inc.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(num_hidden_units)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation('relu')(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "# out = Dense(1, activation='sigmoid')(x)\n",
    "# inc = load_model('Finalized Hierarchical Models/bcc_others_inceptionv3_512_full_trainbias-pos25.h5', \n",
    "#                  custom_objects={'f1': f1, 'precision': precision, 'recall': recall})\n",
    "# for _ in range(6):\n",
    "#     inc.layers.pop()\n",
    "print(inc.layers[-1])\n",
    "x = inc.layers[-1].output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(num_hidden_units_1, name='d1')(x)\n",
    "x = BatchNormalization(name='bn1')(x)\n",
    "x = Activation('relu', name='a1')(x)\n",
    "# x = Dropout(0.5, name='do1')(x)\n",
    "\n",
    "# x = Dense(num_hidden_units_1, name='d2')(x)\n",
    "# x = BatchNormalization(name='bn2')(x)\n",
    "# x = Activation('relu', name='a2')(x)\n",
    "x = Dropout(0.2, name='do2')(x)\n",
    "\n",
    "out = Dense(1, activation='sigmoid', name='out')(x)\n",
    "\n",
    "model = Model(inc.input, out)\n",
    "\n",
    "last_weights = model.layers[-1].get_weights()\n",
    "last_weights[1] += 0.666\n",
    "model.layers[-1].set_weights(last_weights)\n",
    "\n",
    "for layer in inc.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(h):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(h.history['loss'], 'r-')\n",
    "    plt.plot(h.history['val_loss'], 'b-')\n",
    "    plt.title('Loss plot')\n",
    "    plt.legend(['Training loss', 'Validation loss'])\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(h.history['f1'], 'r-')\n",
    "    plt.plot(h.history['val_f1'], 'b-')\n",
    "    plt.title('F1 plot')\n",
    "    plt.legend(['Training F1', 'Validation F1'])\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(h.history['precision'], 'r-')\n",
    "    plt.plot(h.history['val_precision'], 'b-')\n",
    "    plt.title('Precision plot')\n",
    "    plt.legend(['Training precision', 'Validation precision'])\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(h.history['recall'], 'r-')\n",
    "    plt.plot(h.history['val_recall'], 'b-')\n",
    "    plt.title('Recall plot')\n",
    "    plt.legend(['Training recall', 'Validation recall'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(6e-5))\n",
    "_ = model.fit_generator(train_gen, \n",
    "              steps_per_epoch=(train_gen.samples//BATCH_SIZE)+1, \n",
    "              validation_data=val_gen, \n",
    "              validation_steps=(val_gen.samples//BATCH_SIZE)+1, \n",
    "              shuffle=True, class_weight=class_weights, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(lr=5e-5, decay=8e-4), \n",
    "              metrics=['accuracy', precision, recall, f1])\n",
    "print('Model compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "stopper = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)\n",
    "checkpoint = ModelCheckpoint(filename, \n",
    "                             monitor='val_f1', \n",
    "                             mode='max', save_best_only=True, \n",
    "                             verbose=1)\n",
    "r = ReduceLROnPlateau(monitor='val_loss', patience=2, factor=0.05)\n",
    "\n",
    "hist = model.fit_generator(train_gen, \n",
    "                           steps_per_epoch=(train_gen.samples//BATCH_SIZE)+1, \n",
    "                           validation_data=val_gen, \n",
    "                           validation_steps=(val_gen.samples//BATCH_SIZE)+1, \n",
    "                           shuffle=True, class_weight=class_weights, \n",
    "                           callbacks=[stopper, checkpoint, r],\n",
    "                           epochs=20, initial_epoch=initial_epoch)\n",
    "\n",
    "make_plots(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "y_true = test_gen.classes\n",
    "y_pred = model.predict_generator(test_gen,\n",
    "                                 verbose=1)\n",
    "\n",
    "print(y_true.shape)\n",
    "print(y_pred.shape)\n",
    "\n",
    "y_pred[y_pred >= 0.5] = 1\n",
    "y_pred[y_pred < 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "'''\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.99      0.91      0.95       222\n",
    "          1       0.62      0.94      0.75        33\n",
    "\n",
    "avg / total       0.94      0.92      0.92       255\n",
    "\n",
    "\n",
    "----------------\n",
    "\n",
    "[[203  19]\n",
    " [  2  31]]\n",
    "'''\n",
    "\n",
    "\n",
    "print(classification_report(y_true=y_true.ravel(), y_pred=y_pred.ravel()))\n",
    "print('\\n----------------\\n')\n",
    "print(confusion_matrix(y_true=y_true.ravel(), y_pred=y_pred.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.98      0.93      0.95       222\n",
    "          1       0.64      0.88      0.74        33\n",
    "\n",
    "avg / total       0.94      0.92      0.93       255\n",
    "\n",
    "\n",
    "----------------\n",
    "\n",
    "[[206  16]\n",
    " [  4  29]]\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
